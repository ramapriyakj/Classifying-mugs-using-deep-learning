{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_ml6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramapriyakj/ML6/blob/master/tensorflow_ml6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOrpsuYIUHjP",
        "colab_type": "code",
        "outputId": "50a12ca9-e164-494a-de02-8e4ae3d007a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "data_set_dir = root_dir + 'Colab Notebooks/dataset/data/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfRPgaTvVW0U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def create_data_with_labels(image_dir):\n",
        "    \"\"\"Gets numpy data and label array from images that are in the folders that are\n",
        "    in the folder which was given as a parameter. The folders that are in that folder\n",
        "    are identified by the mug they represent and the folder name starts with the label.\"\"\"\n",
        "    mug_dirs = [f for f in os.listdir(image_dir) if not f.startswith('.')]\n",
        "    mug_files = []\n",
        "\n",
        "    for mug_dir in mug_dirs:\n",
        "        mug_image_files = [image_dir + mug_dir + '/' + '{0}'.format(f)\n",
        "                           for f in os.listdir(image_dir + mug_dir) if not f.startswith('.')]\n",
        "        mug_files += [mug_image_files]\n",
        "\n",
        "    num_images = len(mug_files[0])\n",
        "    images_np_arr = np.empty([len(mug_files), num_images, 64, 64, 3], dtype=np.float32)\n",
        "\n",
        "    for mug, _ in enumerate(mug_files):\n",
        "        for mug_image in range(num_images):\n",
        "            img = cv2.imread(mug_files[mug][mug_image])\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img = img.astype(np.float32)\n",
        "            images_np_arr[mug][mug_image] = img / 255.\n",
        "\n",
        "    data = images_np_arr[0]\n",
        "    labels = np.full(num_images, int(mug_dirs[0][0]))\n",
        "\n",
        "    for i in range(1, len(mug_dirs)):\n",
        "        data = np.append(data, images_np_arr[i], axis=0)\n",
        "        labels = np.append(labels, np.full(num_images, int(mug_dirs[i][0])), axis=0)\n",
        "\n",
        "    return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0B2_LhutgVu",
        "colab_type": "code",
        "outputId": "6bd97c43-2f0a-4713-a25b-11430e9fe70a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def read_train_data():\n",
        "  global train_data\n",
        "  global train_labels  \n",
        "  (train_data, train_labels) = create_data_with_labels(data_set_dir+\"train/\")\n",
        "  print(\"Train \",train_data.shape,train_labels.shape)\n",
        "\n",
        "def read_eval_data():\n",
        "  global eval_data\n",
        "  global eval_labels\n",
        "  (eval_data, eval_labels) = create_data_with_labels(data_set_dir+\"test/\")\n",
        "  print(\"Eval \",eval_data.shape,eval_labels.shape)\n",
        "  \n",
        "read_train_data()\n",
        "read_eval_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train  (2000, 64, 64, 3) (2000,)\n",
            "Eval  (1000, 64, 64, 3) (1000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEwf6xWEADSo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def printResults(history):\n",
        "  loss = history['loss']\n",
        "  accuracy = history['accuracy']\n",
        "  epochs = range(len(loss))\n",
        "  \n",
        "  plt.figure()\n",
        "  plt.plot(epochs, loss, 'b', label='loss')\n",
        "  plt.title('loss')\n",
        "  plt.show()\n",
        "  \n",
        "  plt.figure()\n",
        "  plt.plot(epochs, accuracy, 'b', label='accuracy')\n",
        "  plt.title('accuracy')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbiTIq1xVj_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"This file contains all the model information: the training steps, the batch size and the model iself.\"\"\"\n",
        "import math\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_training_steps():\n",
        "    \"\"\"Returns the number of batches that will be used to train your solution.\n",
        "    It is recommended to change this value.\"\"\"\n",
        "    return 3000\n",
        "\n",
        "def get_batch_size():\n",
        "    \"\"\"Returns the batch size that will be used by your solution.\n",
        "    It is recommended to change this value.\"\"\"\n",
        "    return 20\n",
        "\t\n",
        "def getModel(features):\n",
        "  # Input Layer (a batch of images that have 64x64 pixels and are RGB colored (3)\n",
        "  model = tf.reshape(features[\"x\"], [-1, 64, 64, 3])\n",
        "  \n",
        "  model = tf.layers.Conv2D(128,3,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.Conv2D(128,3,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.MaxPooling2D(2,2)(model)\n",
        "  \n",
        "  model = tf.layers.Conv2D(32,1,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.Conv2D(32,1,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.MaxPooling2D(2,2)(model)\n",
        "  \n",
        "  model = tf.layers.Conv2D(32,2,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.Conv2D(32,2,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.MaxPooling2D(2,2)(model)\n",
        "  \n",
        "  model = tf.layers.Conv2D(32,1,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.Conv2D(32,1,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.MaxPooling2D(2,2)(model)\n",
        "  \n",
        "  model = tf.layers.Flatten()(model)\n",
        "  \n",
        "  model = tf.layers.Dense(512,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.Dropout(0.4)(model)\n",
        "  \n",
        "  model = tf.layers.Dense(128,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.Dropout(0.1)(model)\n",
        "  \n",
        "  model = tf.layers.Dense(64,activation=tf.nn.relu)(model)\n",
        "  model = tf.layers.Dropout(0.1)(model)\n",
        "  \n",
        "  model = tf.layers.Dense(4)(model)\n",
        "  return model\n",
        "\n",
        "def solution(features, labels, mode):\n",
        "    logits = getModel(features)\n",
        "    \n",
        "    predictions = {\n",
        "      \"classes\": tf.argmax(input=logits, axis=1),\n",
        "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
        "    }\n",
        "        \n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
        "      return tf.estimator.EstimatorSpec(mode=mode,predictions=predictions)\n",
        "    \n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
        "\t\t\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
        "        optimizer = tf.contrib.opt.AdamWOptimizer(0.001)\n",
        "        train_op = optimizer.minimize(loss=loss,global_step=tf.train.get_global_step())\n",
        "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
        "      \n",
        "    eval_metric_ops = {\"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions[\"classes\"])}\n",
        "\n",
        "    if mode == tf.estimator.ModeKeys.EVAL:\n",
        "        return tf.estimator.EstimatorSpec(mode=mode,loss=loss,eval_metric_ops=eval_metric_ops)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpMHe2AMWCEx",
        "colab_type": "code",
        "outputId": "ebbbdbe3-4397-44b8-a73f-82b8b0199550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1106
        }
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.training.python.training import hparam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def train_model(params):\n",
        "  \n",
        "    size = len(train_data)\n",
        "    total_size = get_training_steps()*get_batch_size()    \n",
        "    print(\"Size = \",size,\",total_size = \",total_size)\n",
        "    \n",
        "    train_datagen = ImageDataGenerator(width_shift_range=0.4,height_shift_range=0.4,shear_range=0.2,zoom_range=0.1,channel_shift_range=0.2,fill_mode='nearest')\n",
        "    train_generator = train_datagen.flow(train_data,train_labels,batch_size=size)\n",
        "    \n",
        "    td,tl = next(train_generator)\n",
        "    i = size\n",
        "    while i < total_size:\n",
        "        next_td,next_tl = next(train_generator)\n",
        "        td = np.concatenate([td,next_td])\n",
        "        tl = np.concatenate([tl,next_tl])\n",
        "        i += size\n",
        "        print(i,td.shape,tl.shape)\n",
        "        \n",
        "    print(\"The shape is :\",td.shape,tl.shape)\n",
        "    \n",
        "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x={\"x\": td},\n",
        "        y=tl,\n",
        "        batch_size=get_batch_size(),\n",
        "        num_epochs=None,\n",
        "        shuffle=True)\n",
        "    \n",
        "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
        "        x={\"x\": eval_data},\n",
        "        y=eval_labels,\n",
        "        num_epochs=1,\n",
        "        shuffle=False)\n",
        "\n",
        "    estimator = tf.estimator.Estimator(model_fn=solution)\n",
        "\n",
        "    steps_per_eval = int(get_training_steps() / params.eval_steps)\n",
        "    \n",
        "    history = {}\n",
        "    history[\"loss\"] = []\n",
        "    history[\"accuracy\"]= []\n",
        "    for _ in range(params.eval_steps):\n",
        "        estimator.train(train_input_fn, steps=steps_per_eval)\n",
        "        eval_results = estimator.evaluate(eval_input_fn)\n",
        "        print(eval_results)\n",
        "        history[\"loss\"].append(eval_results[\"loss\"])\n",
        "        history[\"accuracy\"].append(eval_results[\"accuracy\"])\n",
        "    printResults(history)\n",
        "        \n",
        "class pp:\n",
        "  eval_steps = 1\n",
        "\n",
        "tf.logging.set_verbosity('ERROR')\n",
        "HPARAMS = pp()\n",
        "train_model(HPARAMS)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Size =  2000 ,total_size =  60000\n",
            "4000 (4000, 64, 64, 3) (4000,)\n",
            "6000 (6000, 64, 64, 3) (6000,)\n",
            "8000 (8000, 64, 64, 3) (8000,)\n",
            "10000 (10000, 64, 64, 3) (10000,)\n",
            "12000 (12000, 64, 64, 3) (12000,)\n",
            "14000 (14000, 64, 64, 3) (14000,)\n",
            "16000 (16000, 64, 64, 3) (16000,)\n",
            "18000 (18000, 64, 64, 3) (18000,)\n",
            "20000 (20000, 64, 64, 3) (20000,)\n",
            "22000 (22000, 64, 64, 3) (22000,)\n",
            "24000 (24000, 64, 64, 3) (24000,)\n",
            "26000 (26000, 64, 64, 3) (26000,)\n",
            "28000 (28000, 64, 64, 3) (28000,)\n",
            "30000 (30000, 64, 64, 3) (30000,)\n",
            "32000 (32000, 64, 64, 3) (32000,)\n",
            "34000 (34000, 64, 64, 3) (34000,)\n",
            "36000 (36000, 64, 64, 3) (36000,)\n",
            "38000 (38000, 64, 64, 3) (38000,)\n",
            "40000 (40000, 64, 64, 3) (40000,)\n",
            "42000 (42000, 64, 64, 3) (42000,)\n",
            "44000 (44000, 64, 64, 3) (44000,)\n",
            "46000 (46000, 64, 64, 3) (46000,)\n",
            "48000 (48000, 64, 64, 3) (48000,)\n",
            "50000 (50000, 64, 64, 3) (50000,)\n",
            "52000 (52000, 64, 64, 3) (52000,)\n",
            "54000 (54000, 64, 64, 3) (54000,)\n",
            "56000 (56000, 64, 64, 3) (56000,)\n",
            "58000 (58000, 64, 64, 3) (58000,)\n",
            "60000 (60000, 64, 64, 3) (60000,)\n",
            "The shape is : (60000, 64, 64, 3) (60000,)\n",
            "{'accuracy': 0.838, 'loss': 0.45904094, 'global_step': 3000}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlMMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TT7n0rY9POuB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8ca5d35a-f9e5-4169-c247-d9e4d99a2e94"
      },
      "source": [
        "a = np.array([[[2,2],[2,2]],[[2,2],[2,2]],[[2,2],[2,2]]])\n",
        "b = np.array([[[2,2],[2,2]],[[2,2],[2,2]]])\n",
        "c = np.concatenate([a,b])\n",
        "print(a.shape,b.shape,c.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 2, 2) (2, 2, 2) (5, 2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}